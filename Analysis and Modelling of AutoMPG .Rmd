---
title: "Project Markdown"
output:
  word_document: default
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction
This project will focalise on a specific dataset, undertaking multiple methods of data analysis and several applications of statistical modelling to identify the best regression model, given the attributes of the response variable. The project will consist of 4 parts: **Exploratory Analysis**, **Simple Linear Regression Analysis**, **Multiple Linear Regression Analysis**, and a **Conclusion**. The dataset I've decided to use is AutoMPG, provided by the lecturer. AutoMPG is a dataset involving specs of specific cars manufactured between 1970 and 1982. It's a modified data set that is taken from the library of and currently maintained by Carnegie Mellon University. We will hone in on specific subgroups that make distinctions between the car characteristics, and specifically each of their individual impacts on car fuel consumption. This is a dataset of interest as there is a distinct response variable to be tested and multiple variations of data types that may have an effect on the response variable (continuous, discrete, string etc.). Furthermore, this study may become of importance to car purchasers or manufacturers in the future who want to identify the most efficient cars that can be utilised in the modern day.

## Objective
Utilising the AutoMPG dataset, I am tasked to identify the relationship between fuel consumption and multiple characteristics of a car. I am to thoroughly investigate the data through exploratory analysis, conduct simple linear regression and multiple linear regression, and decide the most suitable predictive model for each specific instance in this specific dataset. I have to consider and develop a multivariable model between fuel consumption and model year, and indicate any effects of relationships between key explanatory variables.


## Exploratory Analysis

Exploratory Analysis involves the first steps of statistical analysis. It is the best way to understand the variables of the dataset, find points of interests, create tables to indicate significant values and give insight on the approach we should take to apply a statistical model to the data. In other words, we make inferences on how the data appears and the significance of certain variables over others in the dataset. 

## Setting up our response and exploratory variables

As the project involves the effects of car characteristics **on** fuel consumption, we shall consider mpg (the measure of fuel consumption) as the main response variable, and the remaining variables will be considered the explanatory variables. Firstly (assuming data is fully loaded), we must consider the nature of the variables, and then make certain assumptions about the response variable.

## Data Description:
```{r}
(load('AutoMPG.RData'))
```
```{r}
auto <- AutoMPG
head(auto)  #familiarise myself with the data
summary(auto)
str(auto)
dim(auto)
mpg <- AutoMPG$mpg   #separating data
cylinders <- AutoMPG$cylinders
displacement <- AutoMPG$displacement
horsepower <- AutoMPG$horsepower
weight <- AutoMPG$weight
acceleration <- AutoMPG$acceleration
model.year <- AutoMPG$model.year
origin <- AutoMPG$origin
car <- AutoMPG$car
#View(auto)

```
There are 385 observations within 9 variables. All of them are numerical variables except origin and car, which are character variables describing the country of origin of the car and the car name respectively. Cylinders and model year are both categorical variables (as year is a discretisation of time and cylinders are naturally discreet and limited). In other words, exactly 385 cars manufactured between 1970 and 1982 are being displayed, along with certain vehicle characteristics such as weight, horsepower, acceleration, etc. The data types are as follows: mpg: miles per gallon, cylinders: multivalued discrete, displacement: continuous, horsepower: continuous, weight: continuous, acceleration: continuous, model year: multivalued discrete, origin: multivalued discreet, car: unique strings.

## Analysing the Data
We begin by identifying the distribution of each variable present in the dataset. It is optional to use scatterplots, boxplots or histograms, however I've chosen a histogram as it's easier to interpret the general shape of the distributions. Additionally, we create a subgroup for numerical variables below. For now, we consider identifying the data only as univariate. In other words, we only currently focus on the individual values in each variable rather than the comparison of two or more variables in the data set. A barplot is used for the categorical variable origin, which had to be transformed into a table to allow plotting to occur.

```{r}
hist(mpg)
hist(cylinders)
hist(displacement)
hist(horsepower)
hist(weight)
hist(acceleration)
hist(model.year)
origintable <- table(AutoMPG$origin)
barplot(origintable)
originf <- factor(auto$origin, labels = c("American", "Asian", "European"))
#View(originf)
#hist(originf)
#Car isn't measured as it is a unique string.
```
Considering these plot, mpg is considered to have a distribution skewed to the right. This is an important distinction to remember when it comes to exploring the dataset further through linear regression. All continuous (numerical) variables have a distinctive skewed to the right distribution, except for acceleration, which seems to have a fairly normal distribution. 

As for the categorical variables (cylinder, model year, origin), they're also skewed to the right, however we do not interpret the similarly to the numerical variables. For example, origin's skewed distribution only occurs as vehicles are characterised within their manufacture countries.

```{r}
auto1<- (auto[, c(-8,-9)])
pairs(auto1[, -mpg])
pairs(log(auto1[, -mpg]))
```
From an intial exploratory analysis between the explanatory variables and the response variables, there seems to be some indication of skewed distributions and nonconstant variance (looking at the top row). A log transformation appears to strengthen the assumptions of nonconstant variation and normal distribution. This is a possible transformation that we could consider moving forward with the dataset.

Now, we will identify bivariate relationships within the dataset. This is done through correlation charting, in which both, the graphical representation within each set of variables and their correlation coefficients are displayed. We now consider the correlation between numerical variables, excluding categorical variables (model year and cylinders). 
```{r}
autonum <- auto[,c(-2,-7,-8,-9)]
autonum1 <- auto[, c(-8,-9)]  #includes numerical categorical variables.
library(PerformanceAnalytics)
#View(autonum)
pairs(autonum) #appendix
chart.Correlation(autonum, histogram = TRUE, pch = 19)
```
The figure above outputs very interesting results for interpretations. The response variable mpg has a considerable strong negative correlation with all the explanatory variables, except for acceleration. This could be explained due to the normal distribution appearance of acceleration we had viewed above, in which every other variable fitted a skewed to the right distribution, similar to the mpg response variable. The explanatory numerical variable with the strongest correlation with mpg is weight. We should consider this as the best predictor moving forward with our regression analysis. 

```{r}
autocat <- auto[,c(-3,-4,-5,-6,-8,-9)] 
#View(autocat)
pairs(autocat) #APPENDIX
chart.Correlation(autocat)
```
Before we do that, we can also consider the discrete values and their impact on the response variable too, as well as on each other. Again, considering they are discreet variables, it would be inappropriate to test their correlation considering that is only utilised for continuous variables. Even considering their correlation, it is not as strong as the correlation of the weight variable, which seems to be the most appropriate predictor to undergo simple linear regression. The cylinders variables appears to have too little a sample and model year has skewed to the right distribution (indicating most cars in this dataset were produced at the beginning of the 1970-82 period).

```{r}
plot(mpg ~ weight)
```
If we take a closer look at the bivariate relationship between weight and mpg, we can see that although the relationship is moderately to strongly negative (higher values of one variable leads to lower values of the other), the line appears to be non-linear. This indicates that transformation must occur to allow a better fit for the data. In terms of outliers, there aren't any values that significantly disturb the general pattern of the data, as the scatter seems consistent throughout the graph, although there is more scatter occuring at the lower weight than there is at heavier weights. The scale of the plot seems sufficient for the values being presented. We can quickly validate the correlation of these variables through hypothesis testing.

```{r}
plot(log(mpg) ~ log(weight))
```
As previously mentioned, the log transformation appears to eliminate the curve relationship and produce a far more linear relationship.

## Correlation of numerical variables.
When discussing the correlation between the numerical explanatory variables and the response variable, it is important to identify whether assumptions of correlation are met. As previously displayed, mpg follows a skewed distribution, not a Normal distribution, thus violates the assumption of bivariate normality, in which two variables can be correlated dependent on their normal distributions matching up to result in another normal distribution. There are three correlation methods; Pearson, Spearman, and Kendall. They all lie within -1 (extreme negative correlation) and +1 (extreme positive correlation). As Pearson's assumption of bivariate normality is violated, we turn to Spearman. We will consider a single numerical explanatory variable to show the process of the 6 steps of hypothesis testing. Weight is most appropriate.
```{r}
cor.test(mpg,weight, method="spearman", exact=F)
pval = pt(17942350, df = 383, lower.tail = F)
pval
```
We see a correlation coefficient of -0.8864761, which is a fairly strong negative correlation. We may perform a hypothesis testing according to these results.

Step 1:$H_0$: mpg and weight are independent, $H_A$: mpg and weight are correlated.

Step 2: Test statistic = 17942350 (seeing R output)

Step 3: Sampling distribution based on ranks.

Step 4: P-value: 0 (or infinitely small)

Step 5: P-value < 0.05 (5% significance level).

Step 6: We reject the Null hypothesis, there is correlation between the two variables.

As we conclude our exploratory analysis of our dataset, now we must approach simple linear regression, taking into account everything that we have learnt from our explanatory variables and our response variable.

We have successfully identified correlation, however this is different from simple linear regression in the sense that a correlation analysis identifies the strength and direction of a bivariate relationship, while simple linear regression is utilised to identify parameters in a linear model which can estimate the values of a variable in response to another. 

## Simple Linear Regression:
There are a couple things that need to be done in Simple Linear Regression. There has to be univariate modelling of at least one key explanatory model (which was identified in the explanatory anlysis). Furthermore, we must distinguish applications towards differing variables, taking into account any transformations or removal of outliers that may further improve the fit of the regression, extending into diagnostics checking. Although we have considered weight to be the strongest correlated explanatory variable to the mpg, we can further consider it as the best model to pick through our R-squared values when undertaking linear modelling for each numerical preditor variable. The R-squared values gives insight on how much a certain predictor variable can explain the response variable (from 0-100%). We will begin by performing simple linear regression and diagnostics checking on the weight predictor variable, and compare it to the alternative predictor variables.

```{r}
mpgweight.lm <- lm(mpg ~ weight)
summary(mpgweight.lm)
plot(mpg ~ weight, xlab='Weight', ylab = 'MPG', main = 'Linear Model of MPG against weight')
abline(mpgweight.lm, lwd = 2, col = 'darkgreen')
confint(mpgweight.lm)
cor(mpg, weight)
```
As provided from the Linear model above, we have produced a regression slope as well as provided summary statistics to help us further accentuate the relationship between the two variables. We have the y-intercept of 46.5027 and slope (gradient) value of -0.0077. In other words, for any vehicle we may have, the average miles per gallon will be 46.5027 (however this is not interpretable at 0 weight). The slope however indicates that whenever an additional unit of weight is added, it is predicted to reduce 0.0077 miles per gallon for the vehicle. Furthermore, we can calculate the 95% confidence interval of each value. As such, we are 95% confident that the intercept lies between 44.96 and 48.04 while the slope lies between -0.0082 and -0.0072. We can do a hypothesis testing utilising the regression line to verify the relationship between mpg and weight, the null hypothesis being the slope is 0, and the alternative hypothesis being the slop is less than 0. 
DO STEPS
```{r}
pt(-0.0077305/0.0002524, 383, lower.tail = T)
```

As the p-value is far less than 0.05, we reject the null hypothesis and conclude that there is a statistically significant negative relationship between weight and mpg.

Now, we will consider the R-squared values for the simple linear regression model of other predictor variables.
```{r}
mpgdisplacement.lm <- lm(mpg ~ displacement)
summary(mpgdisplacement.lm)
mpghorsepower.lm <- lm(mpg ~ horsepower)
summary(mpghorsepower.lm)
mpgacceleration.lm <- lm(mpg ~ acceleration)
summary(mpgacceleration.lm)
#DO LINEAR GRAPHS FOR EACH
```
We can now compare the R-squared values within the numerical predictor variables. Displacement, horsepower and acceleration variables output an R-squared value of 0.6689, 0.6088 and 0.1758 respectively. It is evident that the R-squared value of weight surpasses all of the other predictor variables, making it the best candidate for the regression modelling (weight achieved an R-squared value of 0.7101, indicating that 71% of the mpg is effectively explained by the weight.) Diagnostics checking can now occur to possible find a better fit for the data.

```{r}
par(mfrow=c(2,2))
plot(mpgweight.lm)
names(mpgweight.lm)  ## checking the output of the model
res1=mpgweight.lm$residuals
std.res1=rstandard(mpgweight.lm)  ## standardised residuals
par(mfrow=c(2,2))  ## plotting 4 plots to checkk normality
hist(res1)
hist(std.res1)
qqnorm(res1)
qqline(res1)
qqnorm(std.res1)
qqline(std.res1)
par(mfrow=c(1,1))
plot(res1, xlab="Weight", ylab="Residuals")
```
Through residual analysis, the histogram displays a slightly skewed to the right, but we can consider the distribution to appear symmetrical, thus normality is satisfied. The quantile plots show that the points mainly stay with the line aside from the larger values which drift off. Considering the residuals, there is clear evidence that the variance of errors increases with x, this causes an issue with our model in terms of avoiding relationships between residual and our explanatory variables.

```{r}
par(mfrow=c(2,2))
plot(res1, xlab="Time", ylab="Residuals") ## Residuals vs Time
plot(std.res1,xlab="Time", ylab="Standardised Residuals")
plot(weight,res1, xlab="Weight", ylab="Residuals") # Residuals vs x
plot(weight,std.res1,  xlab="Weight", ylab="Standardised Residuals")
```
The plot of residuals and standardised residuals show clear evidence of a relationship betwwen the explanatory variable and its variance of errors. This is indicated by the non constant scatter at different values of the weight variable. We must now identify any influencing points and eliminate them to enhance our fit, hopefully resulting in an improved R-squared value. We can preceed this by attempting transformations.

```{r}
logmpgweight.lm <- lm(log(mpg) ~ (weight))
summary(logmpgweight.lm)
plot(log(mpg) ~ (weight), xlab='Weight', ylab = 'MPG', main = 'Linear Model of MPG against weight')
abline(logmpgweight.lm, lwd = 2, col = 'darkgreen')
confint(logmpgweight.lm)
cor(log(mpg), (weight)) #ALL APPENDIX
```
A logarithmic transformation of both variables increased the R-squared value output to 0.7617, significantly improving the fit of the linear regression model we initially displayed. The intercept changes to 4.154 (with 95% confidence of points lying between 4.096 and 4.212) and the slope changes to -0.000354 (with 95% confidence of points lying between -0.000373 and -0.000335). We can confirm the better fit by analysing the residuals for this new model.

```{r}
par(mfrow=c(2,2))
plot(logmpgweight.lm)
names(logmpgweight.lm)  ## checking the output of the model
res1log=logmpgweight.lm$residuals
std.res1log=rstandard(logmpgweight.lm)  ## standardised residuals
par(mfrow=c(2,2))  ## plotting 4 plots to checkk normality
hist(res1log)
hist(std.res1log)
qqnorm(res1log)
qqline(res1log)
qqnorm(std.res1log)
qqline(std.res1log)
par(mfrow=c(1,1))
plot(res1log, xlab="Weight", ylab="Residuals")
```
The histogram appears closer to normal distribution (confirming Normality), the quantile plots fit more points on the line and the scatter is far more constant across the fitted values (bottom left), ie. constant variance. As such, we consider this model more appropriate. We can now look to remove any influential points that significantly impact the fit of the SLR model between our predictor variable and the response variable.

```{r}
logcooksdweight <- cooks.distance(logmpgweight.lm)
plot(logcooksdweight ~ weight, xlab = 'Weight', ylab = "Cook's Distance")
n <- nrow(auto)
abline(h=4/(n-2),  lty = 2)
weightinfluential <- as.numeric(names(sort(logcooksdweight, decreasing = TRUE)[1:19]))
weightinfluential
plot(logcooksdweight ~ weight, xlab = 'Weight', ylab = "Cook's Distance", subset = -weightinfluential)
abline(h=4/(n-2),  lty = 2)
```
We successfully figure out the 19 outliers/leverage points that influence the model. We compare Cook's distances again and we see that the high leverage points are removed. We will see the effect it will have on the linear model, as well as the R-squared output indicating the goodness of the fit.

```{r}
newplot <- plot(log(mpg) ~ log(weight), ylab = "mpg", 
    xlab = "Weight", subset = -weightinfluential)  # remove the outliers
lognewmpgweight.lm <- lm(log(mpg) ~ log(weight), subset = -weightinfluential)
abline(lognewmpgweight.lm)
summary(lognewmpgweight.lm)
```
Through the log transformation and removal of outliers, diagnostics checking efficiently improved the R-squared value from an initial 0.7101 to a value of 0.8114. In other words, 81% of the mpg is explained by the weight, due to our newer and improved linear regression model. 

## Multiple Linear Regression

For this dataset, we now want to consider the relationship between the response variable and a multitude of explanatory variables. The previous pair scatter plots are not enough to determine the relationships between predictors, which may have a significant effect on the regression model. We can begin our Multiple Linear Regression by first creating a model and then investigating parameter estimates as well as their covariance. 

```{r}
mpg.lm1 <- lm(mpg ~ cylinders + displacement + horsepower + weight + acceleration + model.year)
summary(mpg.lm1)
plot(mpg ~ weight, xlab='Weight', ylab = 'MPG', main = 'Linear Model of MPG against weight')
```
We have created a multivariabe model including year, cylinders, displacement, horsepower, weight and acceleration. The R-squared value indicates that 82.08% of mpg can be identified by this regression model. To interpret the slope, we must consider that all other variables are held constant while in calculation. Our initial regression model is:

mpg = -12.32 - 0.4863 Cylinders + 0.0051 Displacement + 0.004435 Horsepower - 0.0066 Weight + 0.02260 Acceleration + 0.7404 Years. 

We can consider acceleration as the most statistically significant predictor variable in relation to the response variable due to its regression coefficient (slope). Matrix implementation of the regression model has also been done, solving for variance (which is possibly useful for anova F-testing).
```{r}
auto1<- (auto[, c(-8,-9)]) #removes character variables
mlrmatrix<- as.matrix(auto1[, -1])
mlrmatrix <- cbind(1, mlrmatrix)
Bhat<- solve(t(mlrmatrix) %*% mlrmatrix) %*% t(mlrmatrix)%*% auto1[, 1]
round(Bhat, 6)
print(solve(t(mlrmatrix) %*% mlrmatrix), digits = 4)
sigma <- summary(mpg.lm1)$sigma
round(sigma * sqrt(diag(solve(t(mlrmatrix) %*% mlrmatrix))), 6)
```
We will not alter or remove less statistically significant variables at this time. We must now consider the trade off between limiting the sum of squares error and the number of explanatory variables present in the regression model. Removing statistically insignificant variables will decrease error however it is necessary to distinguish whether all predictor variables must be present to allow the most accurate MLR Regression model. This can be applied through ANOVA analysis and Partial F tests, which compares the ratios of the sum of squares. We will consider horsepower as the variable to be omitted and tested against for this F-test. Before we begin, we must undergo analysis of variance of the initial MLR model to prove at least one variable is statistically significant in the regression model. 

```{r}
mpg.lmm=lm(mpg~1,data=auto1)  ## model with intercept only
anova(mpg.lmm, mpg.lm1)
```

The null hypothesis is that all intercepts equal zero while the alternative hypothesis states the values are not 0. Due to the p-value below the 5% significance level, we reject the null hypothesis and conclude that at least one of the variables is statistically significant to the response mpg variable.

The outputted analysis variance (comparing the model with the intercept) indicates the following values: n=385, p=6, SSR=19358,F=288.53,p-value < 2.2^10-6. We may now conduct the Partial F-test to consider the additional variable horspower.

```{r}
mpg.lm2 <- lm(mpg ~ cylinders + displacement + weight + acceleration + model.year)
anova(mpg.lm1, mpg.lm2)
```
The P-value (0.7434) fails to reject the null hypothesis (DO THIS), horsepower isn't required in the multiple linear regression model. We can begin the final step of our modelling process, diagnostics checking. This will done through utilising residual analysis.

```{r}
mlrres=mpg.lm1$residuals #Extracting residuals from model
mlrstd.res=rstandard(mpg.lm1)  #residuals are standardised
par(mfrow=c(2,2))  ## Normality and constance variance check
hist(mlrstd.res)
qqnorm(mlrstd.res)
qqline(mlrstd.res)
plot(mlrstd.res,xlab="Time", ylab="Standardised Residuals")
plot(mpg.lm1$fitted.values,mlrstd.res, xlab="Fitted Values", ylab="Standardised Residuals")
par(mfrow=c(1,1))
```
The Normality, constant variance and independence assumptions are met for the residuals of the model. However there does seem to be some relationship between standardised residuals and fitted values. Furthermore, the q-q plot shows more significant residuals at the higher end than the lower. We can further plot standardised residuals against our predictor variables as part of our diagnostics checking.

```{r}
par(mfrow=c(2,4))
plot(auto$cylinder,mlrstd.res,xlab="Cylinder", ylab="Standardised Residuals")
plot(auto$displacement,mlrstd.res,xlab="Displacement", ylab="Standardised Residuals")
plot(auto$horsepower,mlrstd.res,xlab="Horsepower", ylab="Standardised Residuals")
plot(auto$weight,mlrstd.res,xlab="Weight", ylab="Standardised Residuals")
plot(auto$acceleration,mlrstd.res,xlab="Acceleration", ylab="Standardised Residuals")
plot(auto$model.year,mlrstd.res,xlab="Year", ylab="Standardised Residuals")
```

```{r}
par(mfrow=c(2,2))
plot(mpg.lm1)
summary(mpg.lm1)
```
We can determine that outliers must be omitted for a better regression model fit.
```{r}

mlrcooksdist <- cooks.distance(mpg.lm1)
plot(mlrcooksdist ~ mpg)
abline(h=4/(n-2))
mlrinfluential <- as.numeric(names(sort(mlrcooksdist, decreasing = TRUE)[1:10]))
mlrinfluential
plot(mlrcooksdist ~ mpg, xlab='mpg', ylab="Cook's Dist", subset = -mlrinfluential)
abline(h=4/(n-2),  lty = 2)
```
We omit the top 10 outliars. 
```{r}

rmoutliermpg.lm1 <- lm(mpg ~ cylinders + displacement + horsepower + weight + acceleration + model.year, subset = -mlrinfluential)
summary(rmoutliermpg.lm1)
```
Omitting the top 10 outliars has successfully increased the R-squared value, and thus enhanced the fit of the regression model.

Completing diagnostics checking, we may want to see if any categorical value is of interest, specifically, the origin of manufacturer.

```{r}
originfit<-lm(mpg~origin,data=auto)
summary(originfit)
```
The slope demonstrates the difference in the statistical significance of each manufacturer origin and its impact on the mpg response variable. Considering Asian cars have the highest regression coefficient, they are the most statistically significant to the response variable at hand.

Relating back to the main linear regression model and dataset, we must utilise model-building techniques to identify which combination of explanatory variables provide increased predictive ability. All subsets will be utilised as it can optimise any function we want to limit or increase (SSE, R-squared, etc.).

```{r}
require(leaps)
par(mfrow = c(1, 3))
allsubsets <- regsubsets(mpg ~ ., nvmax=10, nbest=1, data = auto1)
summary(allsubsets)
summary.allsubsets <- summary(allsubsets)
plot(1:6, summary.allsubsets$adjr2, xlab = "subset size", ylab = "adjusted R-squared", type = "b")
plot(1:6, summary.allsubsets$cp, xlab = "subset size", ylab = "Mallows' Cp", type = "b")
plot(1:6, summary.allsubsets$bic, xlab = "subset size", ylab = "BIC", type = "b")
```
We can interpret the results of this all-subsets modelling technique. As subset size increases (number of predictor variables in the model), The adjusted R-squared value increases heavily yet slowly decreases as more are added. As subset size increases, the Mallow's CP and BIC are both rapidly decreasing, yet rise as the subset size increases further. BIC rises more significantly than Mallow's CP.

```{r}
require(leaps)
par(mfrow = c(1, 3))
allsubsets <- regsubsets(mpg ~ ., nvmax=10, nbest=1, data = auto1)
summary(allsubsets)
summary.allsubsets <- summary(allsubsets)
plot(1:6, summary.allsubsets$adjr2, xlab = "subset size", ylab = "adjusted R-squared", type = "b", log='y')
plot(1:6, summary.allsubsets$cp, xlab = "subset size", ylab = "Mallows' Cp", type = "b", log='y')
plot(1:6, summary.allsubsets$bic - min(summary.allsubsets$bic) + 0.1,, xlab = "subset size", ylab = "BIC", type = "b", log='y')
```
A logarithmic transformation returns the same values, except the Mallow's CP and BIC increase more rapidly as the subset increases past 2 variables.

Finally, we must consider the correlation between the multiple explanatory variables we have chosen. Multi-collinearity may have adverse impacts on the fitting of regression models that we must consider. Highly correlated variables tend to have inflated variances and biased parameter estimates.

```{r}
library(corrplot)
multico <- cbind(displacement,horsepower,weight,acceleration)
multico2 <- cor(multico)
round(multico2,3)
library(car)
vif(mpg.lm1)
corrplot(cor(auto1,method="spearman"))
```
Considering 5 is the usual cut-off value, we consider the regression coefficient to be highly correlated due to their extremely inflated variance values. Furthermore, the corrplot function has identified strong correlation within the explanatory variables except for acceleration, which was initially determined in the exploratory analysis section and now confirmed.

## Conclusion Discussion
























